---
title: "Eric_Hirsch_622_Assignment_1"
subtitle: "Predicting Sales Data" 
author: "Eric Hirsch"
date: "10/7/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =  FALSE, message = FALSE)
```

```{r, include=FALSE}
library(devtools)
library(roxygen2)
library(Hmisc)
library(psych)
library(tidyverse)
library(skimr)
library(purrr)
library(tidyr)
library(tidyverse)
library(gridExtra)
library(lubridate)
library(fastDummies)
library(data.table)
library(mltools)
library(MASS)
library(car)
library(patchwork)
library(ggthemes)
library(tinytex)
library(stats)
library(ggsci)
library(scales)
library(naniar)
library(caret)
library(pROC)
library(tidyverse)
library(Hmisc)
library(skimr)
#install.packages("devtools")
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
library(psych)
library(patchwork)
library(MASS)
library(lubridate)
library(e1071)
library(caTools)
library(class)
#library(mice)
```
```{r}

dfTrain <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\train.csv")
dfTest <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\test.csv")

```


### 1. Data Exploration


#### A. Summary Statistics

```{r}


summary(dfTrain)
str(dfTrain)
```

```{r}


summary(dfTest)

```
```{r}

dfTrain1 <- EHPrepare_MissingValues_Imputation(dfTrain)
dfTest1 <- EHPrepare_MissingValues_Imputation(dfTest)
```


```{r}
library(stringr)
dfTrain1$Cabin1 <- substr(dfTrain1$Cabin,1,1)
dfTrain1$Cabin2 <- str_sub(dfTrain1$Cabin, - 1, - 1) 

dfTrain2 <- dfTrain1 %>%
  dplyr::select(-Cabin) %>%
  mutate(Transported=ifelse(Transported=="True",1,0))

dfTest1$Cabin1 <- substr(dfTest1$Cabin,1,1)
dfTest1$Cabin2 <- str_sub(dfTest1$Cabin, - 1, - 1) 

dfTest2 <- dfTest1 %>%
  dplyr::select(-Cabin)

dfTrain3 <- dfTrain2
dfTest3 <- dfTest2
```


```{r}


a <- EHSummarize_SingleColumn_Histograms(dfTrain3)
grid.arrange(grobs=a[c(1:6)], ncol=3)

EHExplore_OneContinuousAndOneCategoricalColumn_Boxplots(dfTrain3, "Transported")
#grid.arrange(grobs=b[c(1:8)], ncol=1)

EHSummarize_StandardPlots(dfTrain3, "Transported")

```

```{r}

dfNum <- dfTrain %>%
  dplyr::select(where(is.numeric)) 
dfNum <- na.omit(dfNum)

EHExplore_Multicollinearity(dfNum)
```


```{r}

dfTrain4 <- dfTrain3 %>%
  dplyr::select(-Name, -PassengerId)

PassengerID <- dfTest3$PassengerId
dfTest4 <- dfTest3 %>%
  dplyr::select(-Name, -PassengerId)

dfTrain5 <- EHPrepare_CreateDummies(dfTrain4, "Transported")
dfTest5 <- EHPrepare_CreateDummies(dfTest4, "Transported")

dfTest5 <- cbind(PassengerID, dfTest5)

dfTrain5Num <- dfTrain5 %>%
  dplyr::select(where(is.numeric)) 


```

```{r}


pca <- prcomp(dfNum, center = TRUE,scale. = TRUE)

summary(pca)


```

```{r}

#install_github("vqv/ggbiplot")
library(ggbiplot)
ggbiplot(pca, labels=dfx1$HomePlanet)

```

```{r}


library(factoextra)

fviz_nbclust(dfNum, kmeans, method = "wss")
k2 <- kmeans(dfNum, centers=6, iter.max = 10, nstart = 1)
fviz_cluster(k2, data = dfNum)
```

```{r}

dfNumC <- cbind(dfNum, Cluster = k2$cluster)

dfNumCSum <- dfNumC %>%
  group_by(Cluster) %>%
  dplyr::summarise(across(everything(), mean))

dfNumCSumS <- scale(dfNumCSum)
qq <- as.matrix(dfNumCSumS)

heatmap(qq, Rowv = NA, Colv = NA)

#EHExplore_OneContinuousAndOneCategoricalColumn_Boxplots(dfNumC, "Cluster")
```



```{r}
dfx2RFSmall <- dfTrain5 %>%
  dplyr::filter(row_number() %% 2 ==1)

a <- EHModel_RandomForest(dfx2RFSmall, "Transported", categorical=TRUE)

```

```{r}

#b <- EHModel_SVM(dfx2RFSmall, "Transported", method="radial")
c <- EHModel_Regression_Logistic(dfTrain5, "Transported", returnLM = TRUE)

```

```{r}

rf1 <-train(Transported~., data=dfTrain5, method="rf")
 

model = rf1
predictions <- predict(model,newdata=dfTest5)
predictions <- data.frame(as.vector(predictions))
predictions$PassengerId <- dfTest5$PassengerID
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("PassengerID", "Transported")
write_csv(predictions, "C:\\Users\\erico\\Desktop\\SVMpredictions2.csv")

makePredictions2 <- function(model)
{
predictions <- predict(model,newdata=dfTest5)
predictions <- data.frame(as.vector(predictions))
predictions$PassengerId <- dfTest5$PassengerID
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("PassengerID", "Transported")
write_csv(predictions, "C:\\Users\\erico\\Desktop\\SVMpredictions2.csv")
#write_csv(predictions, "D:\\RStudio\\CUNY_621\\Final\\predictionsABC.csv")
}

#makePredictions2(a$rf)
#makePredictions2(b$svm)
#makePredictions2(c)

```
```{r}

#x <- predict(a$rf, dfTest5)
x <- as.data.frame(x)

```

```{r}
dfx3Small <- dfx3 %>%
  filter(row_number() %% 5 ==1)

EHModel_SVM(dfx3Small, "Transported", method="poly")
```

```{r}
library(factoextra)

fviz_nbclust(dfNum, kmeans, method = "wss")

k2 <- kmeans(dfNum, centers=3, iter.max = 10, nstart = 1)
fviz_cluster(k2, data = dfNum)
```



```{r}



```


#### B. Multicollinearity

We suspect a high degree of multicollinearity among the numeric variables, since they are components of each other - for example, total profits is made up of costs and revenues, while revenues are determined by prices and volume.  We also may assume that order date and shipping date are related, and country and region will also be directly related.

The heatmap below shows the multicollinearity among the economic variables.


```{r}

df2Num <- df2 %>%
  dplyr::select_if(is.numeric)

z <- EHExplore_Multicollinearity(df2Num, printCorrs = TRUE, title="Multicollinearity Among Economic Variables")


```
There are many different strategies we can take with the issue of multicollinearity, but because certain columns completely duplicate the information of other columns, we can't ignore it.  We choose, for now, to retain a minimum of variables - Total Profit (as it summarizes most of the others), and, because the same profit may come from high revenue and high costs or low revenue and low costs, we include Unit Cost as well. (Unit cost has the lowest correlation with Total Profit of all the predictors (r=.51)).

As for dates, we convert order date to an integer representing the number of days that have passed since 1/1/2000.  We also create a new variable, Order.Lag, since the difference between order date and shipping date might be predictive.

Finally, we eliminate country and retain region. This leaves us a dataframe of 8 variables.

```{r}

df2$Order.Date <- as.Date(df2$Order.Date, format="%m/%d/%Y")
df2$Ship.Date <- as.Date(df2$Ship.Date, format="%m/%d/%Y")

df2$Order.Lag <- as.integer(df2$Ship.Date-df2$Order.Date)
df2$OrderDaysSince2000 <- as.integer(df2$Ship.Date-as.Date("2000-01-01"))

df3 <- df2 %>%
    dplyr::select(-Order.Date, -Ship.Date, -Country, -Unit.Price, -Total.Revenue, -Total.Cost, -Units.Sold)
```

```{r}
summary(df3)
```
#### C. Distributions

When we examine the distributions of the numeric variables, we find that Total profit is highly skewed, total cost is somewhat skewed, and the date variables are relatively uniform.  There are many odd gaps in the cost distribution, which appears to be a series of discrete values. We may consider doing a log transformation of profit if need be. Since the data is fabricated, the uniformity of the date distributions suggests to me that these dates are just pulled randomly from a uniform distribution and won't be useful.

Not surprisingly, a boxplot shows a great number of outliers for total profits - this is consistent with the skew in the distribution.

```{r}

a <- EHSummarize_SingleColumn_Histograms(df3)
grid.arrange(grobs=a[c(1:4)])

a <- EHSummarize_SingleColumn_Boxplots(df3)
grid.arrange(grobs=a[2])
```

#### D. Relationships

We can run a regression on total profit just to get an idea of some of the relationships between the numeric and categorical variables.  We can see from this exploration that item types are strongly correlated with profits, as are medium priority items, but nothing else is. Unit cost could not be calculated because of singularities. We know unit cost is not fully correlated with total profit, so it must be fully correlated with another variable or in conjunction with other variables.

```{r}

x <- lm(Total.Profit ~., data=df3)
summary(x)
```
This analysis suggests that item type may be the most reasonable class to predict.  However, region may also work, if it is correlated with some of the other variables besides profit.  We can test this conjecture with some further analysis. 

Bar charts and boxplots show relatively little relationship between region and item type, sales channel, and order priority.

```{r}
a <- EHExplore_TwoCategoricalColumns_Barcharts(df3, "Region")
grid.arrange(grobs=a[c(2)])
grid.arrange(grobs=a[c(3:4)])

```
```{r}

ggplot(df2, aes(Region, Order.Lag)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Region, OrderDaysSince2000)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Region, Unit.Cost)) +
  geom_boxplot() +
  coord_flip()

```


####. E. Choosing item type as the variable to predict

We therefore choose Item Type to predict for this analysis.  As with region, we can ask, "how does it correlate with the non-economic variables?"  Item type shows some relationship with order lag and order date.  However, the most striking relationship is with unit cost.  Now we see the source of the singularity - each item type has one, and only one, unit cost and vice versa.  The two are completely correlated.  When we take the log of unit cost, we see a relatively linear relationship.  Just to be sure, a regression shows an R2 of 1.

```{r}

a <- EHExplore_TwoCategoricalColumns_Barcharts(df3, "Item.Type")
grid.arrange(grobs=a[c(3:4)])

```

```{r}

df2a <- df2

ggplot(df2a, aes(Item.Type, Order.Lag)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2a, aes(Item.Type, OrderDaysSince2000)) +
  geom_boxplot() +
  coord_flip()

df2a$Item.Type <- as.factor(df2a$Item.Type)
ggplot(df2a, aes(Unit.Cost, fct_reorder(Item.Type,
                         Unit.Cost))) +
  geom_point() +
  ggtitle("Item Type and Unit Cost") +
    ylab("Item Type")


df2a$Unit.Cost = log(df2a$Unit.Cost)
df2a$Item.Type <- as.factor(df2a$Item.Type)

ggplot(df2a, aes(Unit.Cost, fct_reorder(Item.Type,
                         Unit.Cost))) +
  geom_point() +
  ylab("Item Type") +
  xlab("Log(Unit Cost)") +
  ggtitle("Item Type and Log of Unit Cost")

```


```{r}
dfx3 <- df3 %>%
  dplyr::select(Unit.Cost, Item.Type)

x <- lm(Unit.Cost ~Item.Type, data=dfx3)
summary(x)

```

With unit cost in the analysis, a machine learning exploration is not justified, since a lookup table in Excel would perform just as well. We will retain total profit, and add Units.Sold, which has little correlation with Unit.Cost.  We remove Unit.Cost, add Units.Sold, dummify the categorical variables and scale all the predictors.

```{r}

z=list("Item.Type")

df3a <- df3 %>%
  dplyr::select(-Unit.Cost)

df3a$Units.Sold <- scale(df2$Units.Sold)

df3b <- EHPrepare_CreateDummies(df3a, exclude=z, dropFirst=TRUE)
df4 <- EHPrepare_ScaleAllButTarget(df3b, "Item.Type")

df4$Total.Profit = log(df4$Total.Profit+10)

```


### 2. Models


#### A. Preparing the data

The data needs to be partitioned into a training set and an evaluation set. We examine our classes in the training set and see that they are relatively uniform.

```{r}
set.seed(042760)
i <- createDataPartition(df4$Item.Type, p=0.80, list=FALSE)
dfEval <- df4[-i,]
dfTrain <- df4[i,]

dfTrain %>% count(Item.Type)

```


#### B. Selecting Models

A number of factors weigh in to our decision of which models to choose.  We know that we have multiple classes to predict, that total profit, a key predictor, is not normally distributed, and that, given the strong match between item type and unit cost on the one hand and total profits and unit costs on the other, classes are likely to be relatively separate. Many of our predictors are categorical so we don't expect strong linear relationships.  The number of categories is small compared to the number of records, so our data is not sparse.  


Random Forest (RF) and multinomial regression (MR) will likely perform well under these conditions, so this is what we choose. MR has the advantages that it may be more interpretable and, because we get probabilities instead of firm classes, it is more flexible.  

We have chosen one parametric (MR) and one non-parametric method (RF). The parametric method will likely be simpler, faster and require less data.  However, it may not create as good a fit with the data.  The nonparametric method (RF) requires more data and will be slower, but will likely create a better fit.  This will lead to more accuracy, (unless the paradigm overfits the data which is more of a concern here than with MR.)


We will use 10-fold cross validation.

```{r, messages=FALSE, echo=FALSE}

tc <- trainControl(method="cv", number=10)
metric <- "Accuracy"

set.seed(042760)
multinom <- train(Item.Type~., data=dfTrain, method="multinom", metric=metric, trControl=tc)
rf <- train(Item.Type~., data=dfTrain, method="rf", metric=metric, trControl=tc)
```

```{r}
summary(multinom)
results <- resamples(list(Multinomial=multinom, RandomForest=rf))
summary(results)
dotplot(results)
```
Random Forest performs quite well.  Mean accuracy is 88% at mtry = 14.  

#### C. Making Predicions

Now we test our random forest model on the evaluation set.  We see that certain classes (beverages, fruits and personal care) are predicted very well, while others (meat, snacks) perform less well.  An analysis of why is beyond the scope of this exercise.

```{r}
print(rf)

```

```{r}
predictions <- predict(rf, dfEval)
x <- factor(dfEval$Item.Type)
confusionMatrix(predictions, x)

dfPred <- as.data.frame(predictions)
ggplot(dfPred, aes(predictions)) +
  geom_bar() +
  coord_flip()
```

Which factors are most important? We can see that total profit is driving the analysis. Decision trees leaving total profit in and taking it out make this more clear.

```{r}
varImp(rf)


dt <- train(Item.Type~., data=dfTrain, method="rpart", metric=metric, trControl=tc)
library(rpart.plot)
rpart.plot(dt$finalModel)

dfTrain2 <- dfTrain %>%
  dplyr::select(-Total.Profit)

dt <- train(Item.Type~., data=dfTrain2, method="rpart", metric=metric, trControl=tc)

library(rpart.plot)
rpart.plot(dt$finalModel)
```


#### D. Analyzing the Smaller Dataset

Now we examine the smaller dataset and make some comparisons. Since the 5000 database is a superset of this one, we would expect similarities. 

Multicollinearity and distributions look the similar. Unit Costs and Item Types continue to match one to one. The standard deviation of Total.Profit is higher, as is the mean.  This dataset looks like a sample of the larger one, wth our sample means and standard deviations further from the population compared to the larger one.  

A distribution of classes also shows how sparse the data has become.  Some classes have 3 or less members and some are missing altogether. These "curse of dimensionality" issues are going to hurt the analysis.

With less data to train on and more variation in the dataset, We expect a loss of predictive power with the drop in n. In fact, our accuracy on the training set degrades to .37% (was 87%) and .28% (was 53%) for RF and MR respectively.  Because the data is so sparse, the classes in the training set don't match those in the evaluation set so we can't do predicitons.

```{r}

df2Small <- read.csv("D:\\RStudio\\CUNY_622\\1\\Salesdata_100.csv")

dfSmall <- df2Small %>%
  dplyr::select(-Order.ID)

summary(dfSmall)
```
```{r}

sd(df2$Total.Profit)
sd(dfSmall$Total.Profit)

mean(df2$Total.Profit)
mean(dfSmall$Total.Profit)

dfSmallNum <- dfSmall %>%
  dplyr::select_if(is.numeric)

z <- EHExplore_Multicollinearity(dfSmallNum, title="Multicollinearity Among Economic Variables")
a <- EHSummarize_SingleColumn_Histograms(dfSmall)
grid.arrange(grobs=a[c(1:6)])

dfSmall$Item.Type <- as.factor(dfSmall$Item.Type)
ggplot(dfSmall, aes(Unit.Cost, fct_reorder(Item.Type,
                         Unit.Cost))) +
  geom_point() +
  ggtitle("Item Type and Unit Cost") +
    ylab("Item Type")

```
```{r}
df2 <-dfSmall

df2$Item.Type <- factor(df2$Item.Type)
df2$Region <- factor(df2$Region)

df2$Order.Date <- as.Date(df2$Order.Date, format="%m/%d/%Y")
df2$Ship.Date <- as.Date(df2$Ship.Date, format="%m/%d/%Y")

df2$Order.Lag <- as.integer(df2$Ship.Date-df2$Order.Date)
df2$OrderDaysSince2000 <- as.integer(df2$Ship.Date-as.Date("2000-01-01"))

df3 <- df2 %>%
    dplyr::select(-Order.Date, -Ship.Date, -Country, -Unit.Price, -Total.Revenue, -Total.Cost, -Units.Sold)


z=list("Item.Type")

df3a <- df3 %>%
  dplyr::select(-Unit.Cost)

df3a$Units.Sold <- scale(df2$Units.Sold)

df3b <- EHPrepare_CreateDummies(df3a, exclude=z, dropFirst=TRUE)
df4 <- EHPrepare_ScaleAllButTarget(df3b, "Item.Type")

df4$Total.Profit = log(df4$Total.Profit+10)

set.seed(042760)
i <- createDataPartition(df4$Item.Type, p=0.80, list=FALSE)
dfEval <- df4[-i,]
dfTrain <- df4[i,]

dfTrain %>% count(Item.Type)

tc <- trainControl(method="cv", number=10)
metric <- "Accuracy"

set.seed(042760)
multinom <- train(Item.Type~., data=dfTrain, method="multinom", metric=metric, trControl=tc)
rf <- train(Item.Type~., data=dfTrain, method="rf", metric=metric, trControl=tc)

```

```{r}

results <- resamples(list(Multinomial=multinom, RandomForest=rf))
summary(results)
dotplot(results)

```
```{r}


predictions <- predict(rf, dfEval)
x <- factor(dfEval$Item.Type)
#confusionMatrix(predictions, x)

dfPred <- as.data.frame(predictions)
ggplot(dfPred, aes(predictions)) +
  geom_bar() +
  coord_flip()
```

#### E. Conclusion

Item types were predicted from other aspects of country sales records, such as total profits, region, unit sales, priority, etc. Random forest outperformed multinomial regression for both datasets.  The smaller dataset suffered from low n and sparseness - with a corresponding drop in accuracy.
