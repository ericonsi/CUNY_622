---
title: "Eric_Hirsch_622_Final_Assignment"
subtitle: "Predicting the Space Titanic Kaggle Competition" 
author: "Eric Hirsch"
date: "12/11/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning =  FALSE, message = FALSE)
```

```{r, include=FALSE}
library(devtools)
library(roxygen2)
library(Hmisc)
library(psych)
library(tidyverse)
library(skimr)
library(purrr)
library(tidyr)
library(tidyverse)
library(gridExtra)
library(lubridate)
library(fastDummies)
library(data.table)
library(mltools)
library(MASS)
library(car)
library(patchwork)
library(ggthemes)
library(tinytex)
library(stats)
library(ggsci)
library(scales)
library(naniar)
library(caret)
library(pROC)
library(tidyverse)
library(Hmisc)
library(skimr)
#install.packages("devtools")
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
library(psych)
library(patchwork)
library(MASS)
library(lubridate)
library(e1071)
library(caTools)
library(class)
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
```

```{r}
dfTable <- read.csv("D:\\Rstudio\\Cuny_622\\Space\\622a.csv")
```


# Discussion
## Introduction

In machine learning, we predict target variables based on input variables. For this final exercise, we will apply various machine learning algorithms to a Kaggle data set (Spaceship Titanic) in order to predict which passengers have been transported to another dimension.

While it’s tempting to throw as many algorithms at the problem as possible to see what sticks, the statistical fact is that while it is rare that a poor model will perform well on a holdout set, the chances of making false conclusions based on performance increases if we simply try one model after another.  Besides, if we don’t understand our model and our data, and the model becomes much more difficult to troubleshoot and maintain.

When choosing models, we are balancing simplicity and complexity, and therefore tendencies to underfit or overfit.  When the relationships in the data are simple and certain statistical conditions are met, parametric methods like OLS work well and have the advantage of being easily interpretable.  If, for example, we are predicting height from weight, the relationship is simple enough that we can create a linear regression model and capture most of the variation that can be explained for these two variables.

When we increase our dimensions and/or complexity of relationships within the dataset, parametric methods are likely to underfit the data.  Even in our simple height and weight example, if the relationship between height and weight varies considerably at lower weights, medium weights and higher weights, spline regression or another nonparametric technique will be necessary. As dimensions and complexity increases, we adopt techniques that are more powerful at morphing the data shape so that we can model the underlying structure, such as trees, SVM and neural nets.

Choosing the more complex algorithm will likely fit the training data better, but may be less interpretable and more subject to overfitting.  With this in mind, each of these techniques has its advantages and disadvantages. In my experience with earlier datasets in this class, trees will pick up autonomous clusters in the data set better than SVMs.  For example, if there were a small but significant anomalous cluster of individuals for whom height and weight were inversely related, trees will incorporate the cluster while SVMs will ignore it.  Of course, clusters like this might signal a missing variable, but not all of the necessary variables will be found in any given data set.  On the other hand, when the relationships are more systematic and class boundaries are clear, SVMs may perform better because the kernel trick allows SVMs to radically change the data shape in order to find the class boundary.  SVMs can also perform better when there is less data.

While decision trees are powerful, they are generally more so when bagged (e.g., Random Forest) or boosted (e.g. xgBoost).  Because xgBoost is an active learner, it will often have the upper hand in fitting the training data.

One of the biggest advantages of neural networks is that they effectively do the feature engineering for you if you can apply enough layers.  They are also subject to the “double descent” phenomenon, which helps with managing underfitting.  However, for a student using a home computer like myself, it’s often impractical to take advantage of these facts as the algorithm would run too long.  Neural networks, like SVMs, also powerfully change the data shape in order to find class boundaries.  

Accurate prediction depends not only on algorithm choice. We also need to engineer features (except possibly in very large neural nets) and tune hyperparameters.  We also need to choose metrics that tell us whether or not our model is effective.

## Prediction using the Kaggle Spaceship Titanic Data Set

For this exercise I’ve chosen a Kaggle Competition – the Kaggle Spaceship data set.  The advantages of using this a competition data set are that we can compare our performance to those of others. Achieving 90% on a holdout set in and of itself tells us nothing - we don’t know if achieving 95% would have been easy or impossible. In this competition, the 2,000 or so submitted accuracies on the leaderboard range from about 76% to 82%, which can give us a good idea of how well our model is working.

The main disadvantages of this data set are that the data is made up and the scenario a bit far-fetched. However, I wanted a data set that had a simple class as a target, as opposed to an image example.  The standard Titanic data set has been over analyzed, so this was one of the few good choices left.

### The Business Problem

In the year 2912, the Spaceship Titanic, an interstellar passenger liner with almost 13,000 passengers on board, collided with a spacetime anomaly hidden within a dust cloud.  Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension.  Our job is to predict which passengers were transported by the anomaly using a set of partial records recovered from the spaceship’s damaged computer system.

### Data Summary and Preparation

The data set consists of 8693 records and 13 variables, including spending on the ship’s various amenities (VR Deck, Spa, Room Service, Food Court, and Shopping Mall), cabin number, whether the individual was traveling with the group, whether the individual was a VIP, planet of origin and destination, and so on. These columns map to some degree with the original Titanic database.

The target variable, Transported, is roughly equally distributed between false (4315) and true (4378).

Some of the numeric variables, particularly spending variables, are highly skewed - most passengers spend no money while a few spend a great deal. We can see that spending on luxuries (the spa, room service, etc.) is strongly negatively correlated with being transported - this supports the supposition that the rich were spared. Spending on more budget--friandly amenities like the food court and shopping mall are also negatively correlated but less so. Age has a small negative correlation as well.

We decide not to log transform the numeric variables as normal distributions for predictors are not required by our models and interpretability suffers.

#### Missing Values

1073, or 12%, of records have missing values. The vast majority of missing values are found in the amenity expenditure columns.  Oddly, the amenity expenditure rows with missing values are completely independent of each other - there are almost no records where more than one of these values is missing.

This may be an artifact of the fact that the data is manufactured.  In order to confirm that there is no systematic relationship between missing data and the target variable, we look at the Chi square between the target and a flag designating missing data.  We do this for each amenity expenditure column and find no relationship between missing data and the target variable. We therefore eliminate rows with missing values for the training set. For the test set, we impute the median.

There is very little, even surprisingly little, multicollinearity in the database.  In the case of variables that track spending on amenities this is most surprising, and may suggest that passengers were working within a budget and only spent money on the activities they liked most.

#### Outliers

All of the spending variables are highly skewed, with very large instances occurring at the very end of the distribution.   However, as most of our techniques are robust for outliers, records with extreme values remain in the database, as there is no reason to think that the spending is a data entry error or an anomalous occurrence.

#### Feature Engineering

The data set holds a number of opportunities for feature engineering. Through testing, it was found that the following new features were significant in predicting transportation.  They are: 

1. Groups - Passenger ID numbers suggest that some passengers boarded the ship as part of a group.  Although we could not establish that members of a group tended to meet the same fate, being a member of a group influenced whether a passenger was transported.

2. Cabins - Cabin codes were parsed for location n the ship - this information was correlated with the target variable.

3. Interaction variables - there were a number of interactions among variables which appeared when the variables were examined in isolation. We only retained one (group passengers were more likely to be transported if they shopped at the mall) because the others did not significantly increase accuracy when running the overall model - but there would be more to explore here.

At this point, a picture of the transported passengers begins to emerge - they are the poorer passengers, most likely to shop on a budget or, even cheaper, spend the trip in cryosleep.  They tend to enter the ship in groups and inhabit lower class cabins.

### Modelling

#### Choosing and Testing Models

Understanding what models are doing and how is a key part of prediction. We compared tree models, svm, neural net, and logistic regression.

The first task is to understand the requirements of the business problem.  In this case, we have a partial roster of passengers where we know who was transported and who wasn’t.  As for the rest of the passengers, it is our job to predict which of them were transported as well.  Insight into the data is necessary insofar as it helps us make better predictions, but we need no more from the data than that. Accuracy is our metric, as this is the metric used in the Kaggle (e.g., there is no penalty for false positives or false negatives which might suggest a different metric).  This suggests we should use the most complex model we can that has the highest accuracy.

We used tenfold cross validation and compared models on the metric of accuracy – our results varied widely.  There was some, but not perfect, match between the accuracy predicted by the cross validation, and the accuracy achieved in the Kaggle.  For example, going on cross validation alone, the svm with the radial kernel underperformed compared to a linear and polynomial kernel - however, in the Kaggle it significantly outperformed both kernels.  Of course, without the Kaggle we would not have known this and would not have chosen the radial kernel to put into production.

On the other hand, our best model on cross validation (xgboost) also performed best on the Kaggle.  We suspect the neural net would have performed well also, but we did not have the computer power to hypertune it properly.

#### Hyperparameter Tuning

The caret package in R automatically hypertunes models on basic parameters and chooses the most optimal based on the metric determined by the user. With the exception of xgBoost, all of our models were tuned that way.  This left two tasks – tune xgBoost, and experiment with manual tuning of other models.

The tuned xgBoost model (number of rounds was reduce from 55 to 14) increased accuracy from 79% to 80%, and improved our percentile position in the Kaggle from the 73rd to the 78th percentile.  

The fact that our radial-kernal SVM had lower cross validation accuracy but higher Kaggle accuracy might have a number of different causes - but one is the possibility that there are idiosyncrasies in the training set that are not found in the Kaggle set.  In this case, we can increase sigma to decrease the risk of overfitting.   

As we experimented by increasing sigma and decreasing sigma on the radial SVM model.  Reducing sigma (tighter fit) improved accuracy during cross validation, but not when applied to the Kaggle.  Because the differences were very small (.0026 improvement in accuracy), the models probably only varied by a handful of predictions. Increasing sigma (looser fit) reduced accuracy both in cross validation and on the Kaggle set.

### Results

```{r}

knitr::kable(dfTable)

```

## Discussion

For this exercise, we entered a Kaggle competition (Space Titanic) in order to make predictions and test their accuracy against other competition participants.  The target variable was whether or not a passenger was transported to another dimension while the ship was moving through a dangerous and destructive space anomaly.  We reached the 78th percentile, with an idea of further modifications which might help us do better.

In a sense, measuring performance in a Kaggle is a cheat - after a time the “unknown” data in the Kaggle test set becomes increasingly “known” as you test more and more models against it. However, it also provides a real-world check on how well your model is actually performing.

After an initial logistic regression, we improved performance in three ways:

1. Feature Engineering
2. Model Selection
3. Hypertuning

The exercise highlighted the importance of Understanding the data in solid and dependable prediction. First, we needed to clean the data of anomalies (missing values, outliers, etc.). In our case, missing values appear to be at random and outliers didn’t present a problem.

Second, an understanding of the data highly conditioned our ability to perform feature engineering. Transported passengers tended to spend their money at the food court rather than get room service, they occupied lower-class cabins, and many of them were in cryosleep during the journey.  Knowing this helped us parse out cabin locations and discover interaction terms within the data.

All of our models came in between 77% and 80% accuracy (based on a no information rate of 51%) after tenfold cross validation and optimal hypertuning.  This was also the range for individuals who submitted predictions in the Kaggle competition - and so a score of 77% achieved a much lower percentile in the competition than a score a few percentage points higher.  Without the Kaggle, we still would have chosen the xgBoost model, but if we were going to choose an SVM we would likely have chosen a less than optimal kernel.

While it makes sense that the boosted tree model outperforms the bagged tree model, it more difficult to speculate on why tree models perform better than the Support Vector Machines.  Given the small difference in accuracy between them (.003), it is possible that there is no significant difference, and that with slightly different feature engineering the SVM would be the better model.

There would be a number of ways to improve accuracy even further:

1.	Search for more features, particularly interaction features between cryosleep and spending, and understanding better how groupings and cabins work.
2.	Borrow a more powerful computer to perform a properly tuned neural net, and then create a prediction set based on neural net, xgBoost and radial SVM.
3.	Experiment more with manual tuning of models.

In the meantime, however, we are satisfied with the nearly 80th percentile compared to the 26th percentile using logistical regression which we would have achieved prior to this class.

```{r}

#dfTrain <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\train.csv")
#dfTest <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\test.csv")

dfTrain <- read.csv("D:\\Rstudio\\Cuny_622\\Space\\train.csv")
dfTest <- read.csv("D:\\Rstudio\\Cuny_622\\Space\\test.csv")


dfTrain$Transported = ifelse(dfTrain$Transported=="True",1,0)

#dfTrain <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\train.csv")
#dfTest <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\test.csv")
```

# Code

## 1. Data Exploration

### A. Summary Statistics

The data consists of 8693 records and 14 variables (6 numeric nd 8 character). There are a number of missing values and what appear to be skewed distributions among the numeric variables.

```{r}
summary(dfTrain)
str(dfTrain)
dfTrain %>%
  count(dfTrain$Transported)
```


### B. Distributions: Skewedness and Outliers

We look at the distribution of all numeric variables.  Spending variables are highly skewed - most passengers spend no money while a few spend a great deal. We can see that spending on luxuries (the spa, room service, etc.) is strongly negatively correlated with being transported - this supports the supposition that the rich were spared. Spending on more popular amenities like the food court and shopping mall are also negatively correlated but less so. Age has a small negative correlation as well.

We decide not to log transform the numeric variables as normal distributions for predictors are not required by our models and interpretability suffers.

```{r}
dfTrainD <- dfTrain

EHSummarize_StandardPlots(dfTrainD, "Transported")


```

Here we look at count plots for character variables.  Home and destination have an association with transported, but cryoSleep is especially important - over 75% of those in cryosleep (presumably a low budget option as food, drink and space are limited) were transported.

```{r}

dfCount <- dfTrainD %>%
  dplyr::select(HomePlanet, CryoSleep, VIP, Destination, Transported)

dfCount$Transported <- as.character(dfCount$Transported)
EHExplore_TwoCategoricalColumns_Barcharts(dfCount, "Transported")

```
Because we have no reason to think that outliers are errors in data entry or significant data anomalies, and because our algorithms are relatively resistant to outliers, we do not remove outliers from the dataset.

### C. Multicollinearity

While we were aware of the correlations with Transported, it is interesting to note that the correlations among various forms of spending are actually quite mild.  We do not need to address multicollinearity in any systematic way.

```{r}

dfNum <- dfTrain %>%
  dplyr::select(where(is.numeric)) 
dfNum <- na.omit(dfNum)

a <- EHExplore_Multicollinearity(dfNum)
```

### D. Missing Values

Missing values mainly appear for the amenities spending variables in the dataset.  There are over 1,000 (12% of the database). 

```{r}
EHSummarize_MissingValues(dfTrain)
```

The missing values are not correlated with each other, suggesting they are probably missing at random.  To further support this hypothesis we create flags for missing values and perform Chi Square tests against the target variable.  None of the flags are significant.  We will therefore remove the records with missing values from the training set (we will do this after some feature engineering), and impute the median for the test set.  

```{r}

dfMissingRecordsFlagAny <- dfTrain %>%
    mutate(Flag=ifelse(rowSums(is.na(dfTrain)) > 0, 1, 0)) %>%
    dplyr::select(Transported, Flag)

dfMissingRecordsFlag_RoomService <- dfTrain %>%
    mutate(RoomServiceFlag=ifelse(is.na(dfTrain$RoomService) > 0, 1, 0)) %>%
      dplyr::select(Transported, RoomServiceFlag)


dfMissingRecordsFlag_VRDeck <- dfTrain %>%
    mutate(VRDeckFlag=ifelse(is.na(dfTrain$VRDeck) > 0, 1, 0)) %>%
      dplyr::select(Transported, VRDeckFlag)


dfMissingRecordsFlag_SPA <- dfTrain %>%
    mutate(SpaFlag=ifelse(is.na(dfTrain$Spa) > 0, 1, 0)) %>%
      dplyr::select(Transported, SpaFlag)


dfMissingRecordsFlag_ShoppingMall <- dfTrain %>%
    mutate(ShoppingMallFlag=ifelse(is.na(dfTrain$ShoppingMall) > 0, 1, 0)) %>%
      dplyr::select(Transported, ShoppingMallFlag)


dfMissingRecordsFlag_FoodCourt <- dfTrain %>%
    mutate(FoodCourtFlag=ifelse(is.na(dfTrain$FoodCourt) > 0, 1, 0)) %>%
      dplyr::select(Transported, FoodCourtFlag)

print(chisq.test(table(dfMissingRecordsFlagAny)))
print(chisq.test(table(dfMissingRecordsFlag_SPA)))
print(chisq.test(table(dfMissingRecordsFlag_FoodCourt)))
print(chisq.test(table(dfMissingRecordsFlag_VRDeck)))
print(chisq.test(table(dfMissingRecordsFlag_ShoppingMall)))
print(chisq.test(table(dfMissingRecordsFlag_RoomService)))

```

### E. First Pass Logistic Regression:  9th percentile

We perform a logistic regression with what we have and post to Kaggle just to get a baseline.  Accuracy on training is 77%, significantly better than the 51% no information rate, but gives us only 69% on the Kaggle set (which puts us at the 9th percentile).

```{r}


dfTrainNum <- dfTrain %>%
  dplyr::select(where(is.numeric)) 

dfTestNum <- dfTest %>%
  dplyr::select(where(is.numeric))

dfTestNum <- cbind(dfTestNum, "PassengerID" = dfTest$PassengerId)

q <- EHModel_Regression_Logistic(dfTrainNum, "Transported", returnLM = TRUE)

predictions_logistic <- EHModel_Predict(q, dfTestNum, threshold=.5, testData_IDColumn = "PassengerID", predictionsColumnName = "Transported")

predictions_logistic$Transported <- ifelse(predictions_logistic$Transported==1,"True","False")
write_csv(predictions_logistic, "C://Users//erico//Desktop//LR_FirstPass.csv")

```
## Data Preparation and Feature Engineering

### 1. Create groups based on the Passenger ID

Passenger IDs are constructed to identify passengers travelling in groups.  We create groupings from the ID.

How likely is it that if the majority of members of a group were transported, then they all were transported? Only somewhat likely.  A histogram shows the distribution of percentages of transported within groups. Most often, half the members transported and half did not.  

```{r}

Group2 <- dfTrain %>% dplyr::select(PassengerId, Transported) 

Group2$Group <- sub("\\_.*", "", Group2$PassengerId)

Group2Sum <- Group2 %>%
  dplyr::group_by(Group) %>%
  dplyr::summarize(PercentageTransported= sum(Transported)/dplyr::n(), count=dplyr::n())

Group2SumGroups <- Group2Sum %>%
  filter(count>1)

hist(Group2SumGroups$PercentageTransported)


dfTrainA <- dfTrain

dfTrainA$Group <- Group2$Group

dfTrainAA <- inner_join(dfTrainA, Group2Sum, by="Group") %>% dplyr::select(-Group, -PercentageTransported)

dfTrainBB <- dfTrainAA 

#boxplot(as.numeric(dfTrainBB$count), as.numeric(dfTrainBB$Transported))

Group2a <- dfTest %>% dplyr::select(PassengerId) 

Group2a$Group <- sub("\\_.*", "", Group2a$PassengerId)

Group2aSum <- Group2a %>%
  dplyr::group_by(Group) %>%
  dplyr::summarize(count=dplyr::n())

dfTestA <- dfTest

dfTestA$Group <- Group2a$Group

dfTestAA <- inner_join(dfTestA, Group2aSum, by="Group") %>% dplyr::select(-Group)

dfTrainAA$InAGroup = ifelse(dfTrainAA$count>1,1,0)
dfTestAA$InAGroup = ifelse(dfTestAA$count>1,1,0)

dfTrain1 <- na.omit(dfTrainAA)
dfTest1 <- EHPrepare_MissingValues_Imputation(dfTestAA, impute="median")

```

### 2. Create Cabin Variables

Cabin variables consist of 3 parts in the form of a/b/c which indicate the location of the cabin on the ship. Here we extract out parts "a" and "c" - b appears to have no influence on the target.

```{r}
library(stringr)
dfTrain1$Cabin1 <- substr(dfTrain1$Cabin,1,1)
dfTrain1$Cabin2 <- str_sub(dfTrain1$Cabin, - 1, - 1) 

dfTrain2 <- dfTrain1 %>%
  dplyr::select(-Cabin)

dfTest1$Cabin1 <- substr(dfTest1$Cabin,1,1)
dfTest1$Cabin2 <- str_sub(dfTest1$Cabin, - 1, - 1) 

dfTest2 <- dfTest1 %>%
  dplyr::select(-Cabin)

dfTrain3 <- dfTrain2
dfTest3 <- dfTest2
```

### 3. Create Dummy Variables

Now that we have engineered Cabin, we create dummy variables to handle category variables throughout the dataset.

```{r}

dfTrain4 <- dfTrain3 %>%
  dplyr::select(-Name, -PassengerId)

PassengerID <- dfTest3$PassengerId
dfTest4 <- dfTest3 %>%
  dplyr::select(-Name, -PassengerId)

dfTrain5v <- EHPrepare_CreateDummies(dfTrain4, "Transported")
dfTrain5 <- dfTrain5v %>%
  dplyr::select(-Cabin2_)

dfTest5v <- EHPrepare_CreateDummies(dfTest4, "Transported")
dfTest5 <- dfTest5v %>%
  dplyr::select(-Cabin2_)

dfTest5 <- cbind(PassengerID, dfTest5)


```
### 4.  Implement Interaction Features

There were a number of interactions among variables which appeared when the variables were examined in isolation. We only retained one (group passengers were more likely to be transported if they shopped at the mall) because the others did not significantly increase accuracy when running the overall model - but there would be more to explore here.

```{r}

dfTrain5$Inter_CountShop = dfTrain5$InAGroup*dfTrain5$ShoppingMall
dfTest5$Inter_CountShop = dfTest5$InAGroup*dfTest5$ShoppingMall

```



### 5. Perform Logistic Regression With Engineered Features: 26th Percentile

We perform a logistic regression with what we have and post to Kaggle just to get a baseline.  Accuracy on the holdout set improves training to 79% (compared to 77% on the untransformed regression), and gives us 78% on the Kaggle set, which puts us at the 26th percentile.

```{r}
dfTrain5z <- dfTrain5

dfTrain5zz <- dfTrain5z %>%
  dplyr::select(-count)

dfTest5z <- dfTest5

dfTest5zz <- dfTest5z %>%
  dplyr::select(-count)

set.seed(042758)

q <- EHModel_Regression_Logistic(dfTrain5zz, "Transported", returnLM = TRUE)
predictions_logistic <- EHModel_Predict(q, dfTest5zz, threshold=.5, testData_IDColumn = "PassengerID", predictionsColumnName = "Transported")

predictions_logistic$Transported <- ifelse(predictions_logistic$Transported==1,"True","False")
write_csv(predictions_logistic, "C://Users//erico//Desktop//LR_WithFeatures.csv")

```

At this point, a picture of the transported passengers begins to emerge - they are the poorer passengers, most likely to shop on a budget or, even cheaper, spend the trip in cryosleep.  They tend to enter the ship in groups and inhabit lower class cabins.

## More Complex Models

Given the apparent complexity of the data shape, we turn to more complex nonparametric models to improve our predictions. 

### 1. Perform SVM: 70th Percentile

We begin with Support Vector Machines and try three kernels - linear, poly and radial. We perform ten-fold cross validation and optimal hypertuning of C based on the caret package.  Radial performs the best (accuracy=80.1% on the kaggle set) and boosts us to the 70th percentile.

```{r}
library(doParallel)
library(caret)

cl<-makePSOCKcluster(7)
  
registerDoParallel(cl)

print("Linear: --------------------------------------")
lin <- EHModel_SVM(dfTrain5zz, "Transported", method="linear")
print("Poly: --------------------------------------")
poly <- EHModel_SVM(dfTrain5zz, "Transported", method="poly")
print("Radial: --------------------------------------")
rad <- EHModel_SVM(dfTrain5zz, "Transported", method="radial")

predictions_rad <- EHModel_Predict(rad$svm, dfTest5zz, predictionsColumnName = "Transported", testData_IDColumn = "PassengerID")

predictions_rad$Transported <- ifelse(predictions_rad$Transported==1,"True","False")
write_csv(predictions_rad, "C://Users//erico//Desktop//SVM_Rad.csv")


stopCluster(cl)


```


### 2. Perform limited Neural Networks

Neural Networks require a great deal of computer resources and time.  A simple first pass with two hidden layers took a great deal of time, needed a high stepmax to converge and provided poor results (15th percentile).  The algorithm was therefore difficult to hypertune.

```{r}

library(neuralnet)
set.seed(42760)

dfTrainN <- dfTrain5zz
dfTrainN$Transported = as.factor(dfTrainN$Transported)
dfScale <- EHPrepare_ScaleAllButTarget(dfTrainN, "Transported")

#n <- neuralnet(Transported ~ .,
               #data = dfScale,
              # hidden = 2,
               #err.fct = "ce",
              # linear.output = F,
              # lifesign = 'full',
              # rep = 2,
             #  algorithm = "rprop+",
             #  stepmax = 100000, likelihood=TRUE, )

```
```{r}

#plot(n, rep = 2)

```

```{r}
#predicts <- predict(n, dfTest5zz)
#predict_nn <- as.data.frame(predict(n, dfTest5zz)) %>%
#  dplyr::select(V1) %>%
#  dplyr::rename("Transported" = V1) %>%
#  mutate(Transported=as.numeric(Transported)) %>%
#  mutate(Transported=ifelse(Transported>.5,"True","False"))

#predictions_nn <- EHModel_Predict(n, dfTest5zz, predictionsColumnName = "Transported", #testData_IDColumn = "PassengerID")

#write_csv(predictions_nn, "C://Users//erico//Desktop//nn.csv")

```

In order to address the long time until convergence (many hours), we experimented with dimensionality reduction, but this was ineffective.  PCA, e.g., did not result in a small number of components taking the largest share of variance.  Taking a sample of records ormanually eliminating columns allowed for faster run times but hurt performance.  Below is the result of PCA analysis: 

```{r}

pca <- prcomp(dfTrain5zz, center = TRUE, scale. = TRUE)

summary(pca)
plot(pca)

```

### 3. Tree Algorithms 1: Perform Random Forest: 34th Percentile

Random forest is our first tree. We use the parallel library to run the model on multiple cores.

We perform ten-fold cross validation and optimal hypertuning of sigma and c based on the caret package. 

This gives us 79% on the holdout set and 78.8% on the kaggle set, which puts us in the 34th percentile.  This is low compared to SVM.

```{r}
library(doParallel)

cl<-makePSOCKcluster(7)
  
registerDoParallel(cl)


a <- EHModel_RandomForest(dfTrain5zz, "Transported")
predictions_rf <- EHModel_Predict(a$rf, dfTest5zz, predictionsColumnName = "Transported", testData_IDColumn = "PassengerID")

predictions_rf$Transported <- ifelse(predictions_rf$Transported==1,"True","False")
write_csv(predictions_rf, "C://Users//erico//Desktop//RF1.csv")

  
stopCluster(cl)

```

### 4. Tree Algorithms 2: Perform XGBoost Untuned: 73rd Percentile

At the 34th percentile we need a more powerful model.  As an active learner, XGBoost is likely to fit our training model better than random forest, though it may overfit the data.  

Our untuned model, with 55 rounds (chosen randomly), achieves 79% accuracy on the holdout set and 80.2% accuracy on the kaggle set, and reaches the 74th percentile. 

```{r}
library(xgboost)
library(caret)

set.seed(0)

dfTrain5q <-dfTrain5
dfTest5q <- dfTest5

dfTrain5q$Transported = as.character(dfTrain5q$Transported)
dfTrain5$Transported = as.character(dfTrain5$Transported)


train_x <- dfTrain5q %>%
  dplyr::select(-Transported, -count)
train_y <- dfTrain5q %>%
  dplyr::select(Transported)

dfTest6 <- dfTest5q
dfTest6$Transported = "1"

test_x <- dfTest6 %>%
  dplyr::select(-Transported,-PassengerID, -count)
test_y <- dfTest6 %>%
  dplyr::select(Transported)


xgb_train <- xgb.DMatrix(data = as.matrix(train_x), label = as.matrix(train_y))
xgb_test <- xgb.DMatrix(data = as.matrix(test_x), label = as.matrix(test_y))

#depth of 1000 and nrounds 100000 showed no double descent

model2 <- xgb.train(data = xgb_train, max.depth = 3, nrounds = 50)

```

```{r}

predictions <- predict(model2,newdata=xgb_test)
predictions <- data.frame(as.vector(predictions)) 
predictions$PassengerId <- dfTest5$PassengerID
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("PassengerID", "Transported")

predictions <- predictions %>%
  mutate(Transported=ifelse(Transported>.5,'True','False'))

write_csv(predictions, "C:\\Users\\erico\\Desktop\\XGBpredictions_Rounds50.csv")

```

## Hypertuning

Now we try some experiments with hypertuning.  First we choose a more optimal nrounds for xgboost.  Then we try manually adjusting sigma both up and down on our radial SVM model to see if that improves performance.

### 1. Tune XGBoost: 78th Percentile

We perform ten-fold cross validation on 1-100 nrounds and find the optimal nrounds for accuracy (14). Our  model, with 14 rounds, achieves 80.3% accuracy and reaches the 74th percentile. 

```{r, message=FALSE, }
set.seed(100)
cv <- xgb.cv(data = xgb_train, nrounds = 100, nthread = 2, nfold = 10, metrics = list("rmse","auc", "error"),
                  max_depth = 3, eta = 1, objective = "binary:logistic")

trained_model <- xgb.train(data = xgb_train, max_depth = 3,
              eta = 1, nthread = 4, nrounds = 14,
              watchlist = list(train = xgb_train, eval = xgb_test),
              objective = "binary:logistic")


predictions <- predictions %>%
  mutate(Transported=ifelse(Transported>.5,'True','False'))

write_csv(predictions, "C:\\Users\\erico\\Desktop\\XGBpredictions_Rounds14.csv")
```

### 2. Tune SVM Radial: No Improvement

We manually hypertune our svm radial model by increasing sigma (loosening the fit) and decreasing sigma (tightening the fit).  A decreased sigma increased accuracy on the holdout set but not on the Kaggle.

```{r}
library(doParallel)
library(caret)

cl<-makePSOCKcluster(10)
  
registerDoParallel(cl)

#rad3 <- EHModel_SVM(dfTrain5zz, "Transported", method="radial", cValue=1, sigmaValue =.05)

rad3 <- EHModel_SVM(dfTrain5zz, "Transported", method="radial", cValue=1, sigmaValue =.04)
predictions_rad3 <- EHModel_Predict(rad3$svm, dfTest5zz, predictionsColumnName = "Transported", testData_IDColumn = "PassengerID")

predictions_rad3$Transported <- ifelse(predictions_rad3$Transported==1,"True","False")
write_csv(predictions_rad3, "C://Users//erico//Desktop//SVM_Rad_LHigherSigma.csv")

stopCluster(cl)


```

## Discussion

(see above)




