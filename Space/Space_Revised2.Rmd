---
title: "Eric_Hirsch_622_Final_Assignment"
subtitle: "Predicting the Space Titanic Kaggle Competition" 
author: "Eric Hirsch"
date: "12/3/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =  FALSE, message = FALSE)
```

```{r, include=FALSE}
library(devtools)
library(roxygen2)
library(Hmisc)
library(psych)
library(tidyverse)
library(skimr)
library(purrr)
library(tidyr)
library(tidyverse)
library(gridExtra)
library(lubridate)
library(fastDummies)
library(data.table)
library(mltools)
library(MASS)
library(car)
library(patchwork)
library(ggthemes)
library(tinytex)
library(stats)
library(ggsci)
library(scales)
library(naniar)
library(caret)
library(pROC)
library(tidyverse)
library(Hmisc)
library(skimr)
#install.packages("devtools")
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
library(psych)
library(patchwork)
library(MASS)
library(lubridate)
library(e1071)
library(caTools)
library(class)
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
#library(mice)
```

# Summary
## Introduction

In machine learning, we predict target variables based on input variables. For this final exercise, we will apply various machine learning algorithms to a Kaggle data set (Spaceship Titanic) in order to predict which passengers have been transported to another dimension.

While it’s tempting to throw as many algorithms at the problem as possible to see what sticks, the statistical fact is that while it is rare that a poor model will perform well on a holdout set, the chances of making false conclusions based on performance increases if we simply try one model after another.  Besides, if we don’t understand our model and our data, and the model becomes much more difficult to troubleshoot and maintain.

When choosing models, we are balancing simplicity and complexity, and therefore tendencies to underfit or overfit.  When the relationships in the data are simple and certain statistical conditions are met, parametric methods like OLS work well and have the advantage of being easily interpretable.  If, for example, we are predicting height from weight, the relationship is simple enough that we can create a linear regression model and capture most of the variation that can be explained for these two variables.

When we increase our dimensions and/or complexity of relationships within the dataset, parametric methods are likely to underfit the data.  Even in our simple height and weight example, if the relationship between height and weight varies considerably at lower weights, medium weights and higher weights, spline regression or another nonparametric technique will be necessary. As dimensions and complexity increases, we adopt techniques that are more powerful at morphing the data shape so that we can model the underlying structure, such as trees, SVM and neural nets.

Choosing the more complex algorithm will likely fit the training data better, but may be less interpretable and more subject to overfitting.  With this in mind, each of these techniques has its advantages and disadvantages. In my experience with earlier datasets in this class, trees will pick up autonomous clusters in the data set better than SVMs.  For example, if there were a small but significant anomalous cluster of individuals for whom height and weight were inversely related, trees will incorporate the cluster while SVMs will ignore it.  Of course, clusters like this might signal a missing variable, but not all of the necessary variables will be found in any given data set.  Trees may be bagged (e.g., Random Forest) or boosted (e.g. xgBoost) - either will generally perform better than a single decision tree.  Because xgBoost is not a lazy learner, it will often have the upper hand in fitting the training data.
On the other hand, when the relationships are more systematic and class boundaries are clear, SVMs may perform better because the kernel trick allows SVMs to radically change the data shape in order to find the class boundary.  SVMs can also perform better when there is less data.

One of the biggest advantages of neural networks is that they effectively do the feature engineering for you if you can apply enough layers.  They are also subject to the “double descent” phenomenon, which helps with managing underfitting.  However, for a student using a home computer like myself, it’s often impractical to take advantage of these facts as the algorithm would run too long.  Neural networks, like SVMs, also powerfully change the data shape in order to find class boundaries.  

Accurate prediction depends not only on algorithm choice. We also need to engineer features (except possibly in very large neural nets) and tune hyperparameters.  We also need to choose metrics that tell us whether or not our model is effective.

## Prediction using the Kaggle Spaceship Titanic Data Set

For this exercise I’ve chosen a Kaggle Competition – the Kaggle Spaceship data set.  The advantages of using this a competition data set are that we can compare our performance those of others. Achieving 90% on a holdout set in and of itself tells us nothing - we don’t know if achieving 95% would have been easy or impossible. In this competition, the 2,000 or so submitted accuracies on the leaderboard range from about 76% to 82%, which gives us a good idea of how well our model is working.

The main disadvantages of this data set are that the data is made up and the scenario a bit far-fetched. However, I wanted a data set that had a simple class as a target, as opposed to an image example, and the standard Titanic data set has been over analyzed, this was one of the few good choices.

### The Business Problem

In the year 2912, the Spaceship Titanic, an interstellar passenger liner with almost 13,000 passengers on board, collided with a spacetime anomaly hidden within a dust cloud.  Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension.  Our job is to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.

### Data Summary

The data set consists of 8693 records and 13 variables, including spending on the ship’s various amenities (VR Deck, Spa, Room Service, Food Court, Shopping Mall, cabin number, whether the individual was traveling with the group, whether the individual was a VIP, planet of origin and destination, and so on. These columns map to some degree with the original Titanic database.
The target variable, Transported, is roughly equally distributed between false (4315) and true (4378).

#### Distributions

#### Missing Values

1073, or 12%, of records have missing values. The vast majority of missing values are found in the amenity expenditure columns.  Oddly, the amenity expenditure rows with missing values are completely independent of each other - there are no records where more than one of these values is missing.
This may be an artifact of the fact that the data is manufactured.  In order to confirm that there is no systematic relationship between missing data and the target variable, we look at the Chi square between the target and a flag designating missing data.  We do this for each amenity expenditure column and find no relationship between missing data and the target variable.
We therefore eliminate rows with missing values for the training set. The test set, we impute the median.

#### Multicollinearity

There is very little, even surprisingly little, multicollinearity in the database.  In the case of variables that track spending on amenities this is most surprising, and may suggest that passengers were working within a budget and only spent money on the activities they liked most.

#### Outliers

All of the spending variables are highly skewed, with very large ending occurring at the very end of the distribution.   However, as most of our techniques are robust for outliers, records with extreme values remain in the database, as there is no reason to think that the spending is a data entry error or an anomalous occurrence.

### Data Preparation

#### Feature Engineering

The data set holds a number of opportunities for feature engineering. Through testing, it was found that the following new features were significant in predicting transportation.  They are: 

### Modelling

#### Choosing and Testing Models

#### Hyperparameter Tuning

### Results

## Discussion


```{r}

#dfTrain <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\train.csv")
#dfTest <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\test.csv")

dfTrain <- read.csv("D:\\Rstudio\\Cuny_622\\Space\\train.csv")
dfTest <- read.csv("D:\\Rstudio\\Cuny_622\\Space\\test.csv")

dfTrain$Transported = ifelse(dfTrain$Transported=="True",1,0)

#dfTrain <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\train.csv")
#dfTest <- read.csv("C:\\Users\\erico\\Documents\\R\\Space\\test.csv")
```

# Code

## 1. Data Exploration

### A. Summary Statistics

The data consists of 8693 records and 14 variables (6 numeric nd 8 character). There are a number of missing values and what appear to be skewed distributions among the numeric variables.

```{r}
summary(dfTrain)
str(dfTrain)
dfTrain %>%
  count(dfTrain$Transported)
```


### B. Distributions: Skewedness and Outliers

We at the distribution of all numeric variables.  Spending variables are highly skewed - most passengers spend no money while a few spend a great deal. We can see that spending on luxuries (the spa, room service, etc.) is strongly negatively correlated with being transported - this supports the supposition that the rich were spared. Spending on more popular amenities like the food court and shopping mall are also negatively correlated but less so. Age has a small negative correlation as well.

We decide not to log transform the numeric variables as normal distributions for predictors are not required by our models and interpretability suffers.

```{r}
dfTrainD <- dfTrain

EHSummarize_StandardPlots(dfTrainD, "Transported")


```

Here we look at count plots for character variables.  Home and destination have an association with transported, but cryoSleep is especially important - over 75% of those in cryosleep were transported.

```{r}

dfCount <- dfTrainD %>%
  dplyr::select(HomePlanet, CryoSleep, VIP, Destination, Transported)

dfCount$Transported <- as.character(dfCount$Transported)
EHExplore_TwoCategoricalColumns_Barcharts(dfCount, "Transported")

```
Because we have no reason to think that outliers are errors in data entry or significant data anomalies, and because our algorithms are relatively resistant to outliers, we do not remove outliers from the dataset.

### C. Multicollinearity

While we were aware of the correlations with Transported, it is interesting to note that the correlations among various forms of spending are actually quite mild.  We do not need to address multicollinearity in any systematic way.

```{r}

dfNum <- dfTrain %>%
  dplyr::select(where(is.numeric)) 
dfNum <- na.omit(dfNum)

a <- EHExplore_Multicollinearity(dfNum)
```

### D. Missing Values

Missing values mainly appear for the amenities spending variables in the dataset.  There are over 1,000 (12% of the database). 

```{r}
EHSummarize_MissingValues(dfTrain)
```

The missing values are not correlated with each other, suggesting they are probably missing at random.  To further support this hypothesis we create flags for missing values and perform Chi Square tests against the target variable.  None of the flags are significant.  We will therefore remove the records with missing values from the training set (we will do this after some feature engineering), and impute the median for the test set.  

```{r}

dfMissingRecordsFlagAny <- dfTrain %>%
    mutate(Flag=ifelse(rowSums(is.na(dfTrain)) > 0, 1, 0)) %>%
    dplyr::select(Transported, Flag)

dfMissingRecordsFlag_RoomService <- dfTrain %>%
    mutate(RoomServiceFlag=ifelse(is.na(dfTrain$RoomService) > 0, 1, 0)) %>%
      dplyr::select(Transported, RoomServiceFlag)


dfMissingRecordsFlag_VRDeck <- dfTrain %>%
    mutate(VRDeckFlag=ifelse(is.na(dfTrain$VRDeck) > 0, 1, 0)) %>%
      dplyr::select(Transported, VRDeckFlag)


dfMissingRecordsFlag_SPA <- dfTrain %>%
    mutate(SpaFlag=ifelse(is.na(dfTrain$Spa) > 0, 1, 0)) %>%
      dplyr::select(Transported, SpaFlag)


dfMissingRecordsFlag_ShoppingMall <- dfTrain %>%
    mutate(ShoppingMallFlag=ifelse(is.na(dfTrain$ShoppingMall) > 0, 1, 0)) %>%
      dplyr::select(Transported, ShoppingMallFlag)


dfMissingRecordsFlag_FoodCourt <- dfTrain %>%
    mutate(FoodCourtFlag=ifelse(is.na(dfTrain$FoodCourt) > 0, 1, 0)) %>%
      dplyr::select(Transported, FoodCourtFlag)

print(chisq.test(table(dfMissingRecordsFlagAny)))
print(chisq.test(table(dfMissingRecordsFlag_SPA)))
print(chisq.test(table(dfMissingRecordsFlag_FoodCourt)))
print(chisq.test(table(dfMissingRecordsFlag_VRDeck)))
print(chisq.test(table(dfMissingRecordsFlag_ShoppingMall)))
print(chisq.test(table(dfMissingRecordsFlag_RoomService)))

```

### E. First Pass Logistic Regression:  9th percentile

We perform a logistic regression with what we have and post to Kaggle just to get a baseline.  Accuracy on training is 77%, significantly better than the 51% no information rate, but gives us only 69% on the Kaggle set which puts us at the 9th percentile.

```{r}


dfTrainNum <- dfTrain %>%
  dplyr::select(where(is.numeric)) 

dfTestNum <- dfTest %>%
  dplyr::select(where(is.numeric))

dfTestNum <- cbind(dfTestNum, "PassengerID" = dfTest$PassengerId)

q <- EHModel_Regression_Logistic(dfTrainNum, "Transported", returnLM = TRUE)

predictions_logistic <- EHModel_Predict(q, dfTestNum, threshold=.5, testData_IDColumn = "PassengerID", predictionsColumnName = "Transported")

predictions_logistic$Transported <- ifelse(predictions_logistic$Transported==1,"True","False")
write_csv(predictions_logistic, "C://Users//erico//Desktop//LR_FirstPass.csv")

```
## Data Preparation and Feature Engineering

### 1. Create groups based on the Passenger ID

Passenger IDs are constructed to identify passengers travelling in groups.  We create groupings from the ID.

How likely is it that if the majority of members of a group transported, then they all transported? Only somewhat likely.  A histogram shows the distribution of percentages of transported within groups. Most often, half the members transported and half did not.  

```{r}

Group2 <- dfTrain %>% dplyr::select(PassengerId, Transported) 

Group2$Group <- sub("\\_.*", "", Group2$PassengerId)

Group2Sum <- Group2 %>%
  dplyr::group_by(Group) %>%
  dplyr::summarize(PercentageTransported= sum(Transported)/dplyr::n(), count=dplyr::n())

Group2SumGroups <- Group2Sum %>%
  filter(count>1)

hist(Group2SumGroups$PercentageTransported)


dfTrainA <- dfTrain

dfTrainA$Group <- Group2$Group

dfTrainAA <- inner_join(dfTrainA, Group2Sum, by="Group") %>% dplyr::select(-Group, -PercentageTransported)

dfTrainBB <- dfTrainAA 

#boxplot(as.numeric(dfTrainBB$count), as.numeric(dfTrainBB$Transported))

Group2a <- dfTest %>% dplyr::select(PassengerId) 

Group2a$Group <- sub("\\_.*", "", Group2a$PassengerId)

Group2aSum <- Group2a %>%
  dplyr::group_by(Group) %>%
  dplyr::summarize(count=dplyr::n())

dfTestA <- dfTest

dfTestA$Group <- Group2a$Group

dfTestAA <- inner_join(dfTestA, Group2aSum, by="Group") %>% dplyr::select(-Group)

dfTrainAA$InAGroup = ifelse(dfTrainAA$count>1,1,0)
dfTestAA$InAGroup = ifelse(dfTestAA$count>1,1,0)

dfTrain1 <- na.omit(dfTrainAA)
dfTest1 <- EHPrepare_MissingValues_Imputation(dfTestAA, impute="median")

```

### 2. Create Cabin Variables

Cabin variables consist of 3 parts in the form of a/b/c which indicate the location of the cabin on the ship. Here we extract out parts "a" and "c" - b appears to have no influence on the target.

```{r}
library(stringr)
dfTrain1$Cabin1 <- substr(dfTrain1$Cabin,1,1)
dfTrain1$Cabin2 <- str_sub(dfTrain1$Cabin, - 1, - 1) 

dfTrain2 <- dfTrain1 %>%
  dplyr::select(-Cabin)

dfTest1$Cabin1 <- substr(dfTest1$Cabin,1,1)
dfTest1$Cabin2 <- str_sub(dfTest1$Cabin, - 1, - 1) 

dfTest2 <- dfTest1 %>%
  dplyr::select(-Cabin)

dfTrain3 <- dfTrain2
dfTest3 <- dfTest2
```

### 3. Create Dummy Variables

Now that we have engineered Cabin, we create dummy variables to handle category variables throughout the dataset.

```{r}

dfTrain4 <- dfTrain3 %>%
  dplyr::select(-Name, -PassengerId)

PassengerID <- dfTest3$PassengerId
dfTest4 <- dfTest3 %>%
  dplyr::select(-Name, -PassengerId)

dfTrain5v <- EHPrepare_CreateDummies(dfTrain4, "Transported")
dfTrain5 <- dfTrain5v %>%
  dplyr::select(-Cabin2_)

dfTest5v <- EHPrepare_CreateDummies(dfTest4, "Transported")
dfTest5 <- dfTest5v %>%
  dplyr::select(-Cabin2_)

dfTest5 <- cbind(PassengerID, dfTest5)


```
### 4.  Implement Interaction Features

```{r}

dfTrain5$Inter_CountShop = dfTrain5$InAGroup*dfTrain5$ShoppingMall
dfTest5$Inter_CountShop = dfTest5$InAGroup*dfTest5$ShoppingMall

```



### 5. Perform Logistic Regression With Engineered Features: 26th Percentile

We perform a logistic regression with what we have and post to Kaggle just to get a baseline.  Accuracy on training is 79% (compared to 77% on the untransformed training set), but more importantly, this gives us only 78% on the Kaggle set which puts us at the 926th percentile.

```{r}
dfTrain5z <- dfTrain5

dfTrain5zz <- dfTrain5z %>%
  dplyr::select(-count)

dfTest5z <- dfTest5

dfTest5zz <- dfTest5z %>%
  dplyr::select(-count)

q <- EHModel_Regression_Logistic(dfTrain5zz, "Transported", returnLM = TRUE)
predictions_logistic <- EHModel_Predict(q, dfTest5zz, threshold=.5, testData_IDColumn = "PassengerID", predictionsColumnName = "Transported")

predictions_logistic$Transported <- ifelse(predictions_logistic$Transported==1,"True","False")
write_csv(predictions_logistic, "C://Users//erico//Desktop//LR_WithFeatures.csv")

```

## More Complex Models

Given the apparent complexity of the data shape, we turn to more complex nonparametric models to improve our predictions. 

### 1. Perform SVM: 70th Percentile

We begin with Support Vector Machines and try three kernels - linear, poly and radial.  Radial performs the best (accuracy=80.1%) and boosts us to the 70th percentile.

```{r}
library(doParallel)
library(caret)

cl<-makePSOCKcluster(7)
  
registerDoParallel(cl)

print("Linear: --------------------------------------")
#lin <- EHModel_SVM(dfTrain5zz, "Transported", method="linear")
print("Poly: --------------------------------------")
#poly <- EHModel_SVM(dfTrain5zz, "Transported", method="poly")
print("Radial: --------------------------------------")
#rad <- EHModel_SVM(dfTrain5zz, "Transported", method="radial")

predictions_rad <- EHModel_Predict(rad$svm, dfTest5zz, predictionsColumnName = "Transported", testData_IDColumn = "PassengerID")

svmq <- train(Transported~., data=dfTrain5zz, method="svmPoly", preProcess = c("center","scale")) 

predictions_rad$Transported <- ifelse(predictions_rad$Transported==1,"True","False")
write_csv(predictions_rad, "C://Users//eric//Desktop//SVM_Rad.csv")


stopCluster(cl)


```
```{r}


svmq
```


```{r}
library(doParallel)
library(caret)
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)

cl<-makePSOCKcluster(10)
  
registerDoParallel(cl)

#rad2 <- EHModel_SVM(dfTrain5zz, "Transported", method="radial", cValue=1, sigmaValue =.05)
#predictions_rad2 <- EHModel_Predict(rad2$svm, dfTest5zz, predictionsColumnName = "Transported", testData_IDColumn = "PassengerID")

#predictions_rad2$Transported <- ifelse(predictions_rad2$Transported==1,"True","False")
#write_csv(predictions_rad2, "C://Users//Eric//Desktop//SVM_Rad_HigherSigma.csv")

rad3 <- EHModel_SVM(dfTrain5zz, "Transported", method="radial", cValue=1, sigmaValue =.04)
predictions_rad3 <- EHModel_Predict(rad3$svm, dfTest5zz, predictionsColumnName = "Transported", testData_IDColumn = "PassengerID")

predictions_rad3$Transported <- ifelse(predictions_rad3$Transported==1,"True","False")
write_csv(predictions_rad3, "C://Users//Eric//Desktop//SVM_Rad_LHigherSigma.csv")

stopCluster(cl)
rad2

```
### 2. Perform limited Neural Networks

Neural Networks require a great deal of computer resources and time.  A simple first pass with two hidden layers took a great deal of time, needed a high stepmax to converge and provided poor results (15th percentile).  The algorithm was therefore difficult to hypertune.

```{r}

library(neuralnet)
set.seed(42760)

dfTrainN <- dfTrain5zz
dfTrainN$Transported = as.factor(dfTrainN$Transported)
dfScale <- EHPrepare_ScaleAllButTarget(dfTrainN, "Transported")

#n <- neuralnet(Transported ~ .,
               #data = dfScale,
              # hidden = 2,
               #err.fct = "ce",
              # linear.output = F,
              # lifesign = 'full',
              # rep = 2,
             #  algorithm = "rprop+",
             #  stepmax = 100000, likelihood=TRUE, )

```
```{r}

#plot(n, rep = 2)

```

```{r}
#predicts <- predict(n, dfTest5zz)
#predict_nn <- as.data.frame(predict(n, dfTest5zz)) %>%
#  dplyr::select(V1) %>%
#  dplyr::rename("Transported" = V1) %>%
#  mutate(Transported=as.numeric(Transported)) %>%
#  mutate(Transported=ifelse(Transported>.5,"True","False"))

#predictions_nn <- EHModel_Predict(n, dfTest5zz, predictionsColumnName = "Transported", #testData_IDColumn = "PassengerID")

#write_csv(predictions_nn, "C://Users//erico//Desktop//nn.csv")

```

In order to address the long time until convergence (many hours), we experimented with dimensionality reduction, but this was ineffective.  PCA, e.g., did not result in a small number of components taking the largest share of variance.  Taking a sample of records or  manually eliminating columns allowed for faster run times but hurt performance.  Below is the result of PCA analysis: 

```{r}

pca <- prcomp(dfTrain5zz, center = TRUE, scale. = TRUE)

summary(pca)
plot(pca)

```

### 3. Tree Algorithms 1: Perform Random Forest: 34th Percentile

We begin with tree models.  Random forest is our first. We use the parallel library to run the model on multiple cores.

This improves accuracy on the test set to 78.7% which puts us in the 34th percentile

```{r}
library(doParallel)

cl<-makePSOCKcluster(7)
  
registerDoParallel(cl)


a <- EHModel_RandomForest(dfTrain5zz, "Transported")
predictions_rf <- EHModel_Predict(a$rf, dfTest5zz, predictionsColumnName = "Transported", testData_IDColumn = "PassengerID")

predictions_rf$Transported <- ifelse(predictions_rf$Transported==1,"True","False")
write_csv(predictions_rf, "C://Users//erico//Desktop//RF1.csv")

  
stopCluster(cl)

```

### 4. Tree Algorithms 2: Perform XGBoost Untuned: 73rd Percentile

At the 34th percentile we need a more powerful model.  As an active learner, XGBoost is likely to fit our training model better than random forest, though it may overfit the data.  Our untuned model, with 55 rounds, achieves 80.2% accuracy and reaches the 74th percentile. 

```{r}
library(xgboost)
library(caret)

set.seed(0)

dfTrain5q <-dfTrain5
dfTest5q <- dfTest5

dfTrain5q$Transported = as.character(dfTrain5q$Transported)
dfTrain5$Transported = as.character(dfTrain5$Transported)


train_x <- dfTrain5q %>%
  dplyr::select(-Transported, -count)
train_y <- dfTrain5q %>%
  dplyr::select(Transported)

dfTest6 <- dfTest5q
dfTest6$Transported = "1"

test_x <- dfTest6 %>%
  dplyr::select(-Transported,-PassengerID, -count)
test_y <- dfTest6 %>%
  dplyr::select(Transported)

#define final training and testing sets
xgb_train <- xgb.DMatrix(data = as.matrix(train_x), label = as.matrix(train_y))
xgb_test <- xgb.DMatrix(data = as.matrix(test_x), label = as.matrix(test_y))

#tRIAL AND ERROR GAVE 14 AS BEST with max.depth at 3 - COULD TRY OTHERS
#depth of 1000 and nrounds 100000 showed no double descent

model2 <- xgb.train(data = xgb_train, max.depth = 3, nrounds = 50)

```

```{r}

predictions <- predict(model2,newdata=xgb_test)
predictions <- data.frame(as.vector(predictions)) 
predictions$PassengerId <- dfTest5$PassengerID
predictions[,c(1,2)] <- predictions[,c(2,1)]
colnames(predictions) <- c("PassengerID", "Transported")

predictions <- predictions %>%
  mutate(Transported=ifelse(Transported>.5,'True','False'))

write_csv(predictions, "C:\\Users\\erico\\Desktop\\XGBpredictions_Rounds50.csv")

```

## Hypertuning

### 1. Perform XGBoost Tuned: 78th Percentile

1. Define business problem better
2. Interpretation vs predicition - value of knowing the data
3. Explain what was tuned - tune the svm

```{r}
set.seed(100)
cv <- xgb.cv(data = xgb_train, nrounds = 100, nthread = 2, nfold = 10, metrics = list("rmse","auc", "error"),
                  max_depth = 3, eta = 1, objective = "binary:logistic")
print(cv)
print(cv, verbose=TRUE)


# Train a model using 14 rounds (corresponds to best iteration)
trained_model <- xgb.train(data = xgb_train, max_depth = 3,
              eta = 1, nthread = 4, nrounds = 14,
              watchlist = list(train = xgb_train, eval = xgb_test),
              objective = "binary:logistic")
# Get predictions
head(predict(trained_model, xgb_train))
```

