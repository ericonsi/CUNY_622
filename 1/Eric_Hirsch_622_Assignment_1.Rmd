---
title: "Eric_Hirsch_622_Assignment_1"
subtitle: "Predicting Sales Data" 
author: "Eric Hirsch"
date: "10/7/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =  FALSE, message = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
#devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
library(psych)
library(patchwork)
library(tidytable)
library(MASS)
library(lubridate)
library(e1071)
library(caTools)
library(class)
#library(mice)
```
```{r}

df2 <- read.csv("D:\\RStudio\\CUNY_622\\1\\Salesdata_5000.csv")

```

Summary - We analyze sales data records and, after carefully exploring and wrangling the data, choose to predict Item Type using the Multinomial Regression and Random Forest machine learning methods.  Both methods perform well on a smaller dataset, with the Random Forest algorithm giving us 87% accuracy.The smaller dataset matches the larger one closely in terms of standard deviations, means and distributions. However, RF benefits from the higher n and gives us 98% accuracy in our predictions.

### 1. Data Exploration


#### A. Summary Statistics

For this exercise we will examine the 5,000 record and 50,000 record datasets from the assignment website. 

The datasets contain fabricated sales orders generated by VBA for the purpose of practicing analysis.  There are 14 columns, including 7 numeric columns, 5 character and two date. One of the predictors is an ID so we drop it. Here is a summary of the remaining 13 variables:

```{r}

df2 <- df2 %>%
  dplyr::select(-Order.ID)

summary(df2)
str(df2)

df2$Item.Type <- factor(df2$Item.Type)
df2$Region <- factor(df2$Region)

```

#### B. Multicollinearity

We suspect a high degree of multicollinearity among the numeric variables, since they are components of each other - for example, total profits is made up of costs and revenues, while revenues are determined by prices and volume.  We also may assume that order date and shipping date are related, and country and region will also be directly related.

The heatmap below shows the multicollinearity among the economic variables.


```{r}

df2Num <- df2 %>%
  dplyr::select_if(is.numeric)

z <- EHExplore_Multicollinearity(df2Num, printCorrs = TRUE, title="Multicollinearity Among Economic Variables")


```
There are many different strategies we can take with the issue of multicollinearity, but because certain columns completely duplicate the information of other columns, we can't ignore it.  We choose, for now, to retain a minimum of variables - Total Profit (as it summarizes most of the others), and, because the same profit may come from high revenue and high costs or low revenue and low costs, we include Unit Cost as well. (Unit cost has the lowest correlation with Total Profit of all the predictors (r=.51)).

As for dates, we convert order date to an integer representing the number of days that have passed since 1/1/2000.  We also create a new variable, Order.Lag, since the difference between order date and shipping date might be predictive.

Finally, we eliminate country and retain region. This leaves us a dataframe of 8 variables.

```{r}

df2$Order.Date <- as.Date(df2$Order.Date, format="%m/%d/%Y")
df2$Ship.Date <- as.Date(df2$Ship.Date, format="%m/%d/%Y")

df2$Order.Lag <- as.integer(df2$Ship.Date-df2$Order.Date)
df2$OrderDaysSince2000 <- as.integer(df2$Ship.Date-as.Date("2000-01-01"))

df3 <- df2 %>%
    dplyr::select(-Order.Date, -Ship.Date, -Country, -Unit.Price, -Total.Revenue, -Total.Cost, -Units.Sold)
```

```{r}
summary(df3)
```
#### C. Distributions

When we examine the distributions of the numeric variables, we find that Total profit is highly skewed, total cost is somewhat skewed, and the date variables are relatively uniform.  There are many odd gaps in the cost distribution, which appears to be a series of discrete values. We may consider doing a log transformation of profit if need be. Since the data is fabricated, the uniformity of the date distributions suggests to me that these dates are just pulled randomly from a uniform distribution and won't be useful.

Not surprisingly, a boxplot shows a great number of outliers for total profits - this is consistent with the skew in the distribution.

```{r}

a <- EHSummarize_SingleColumn_Histograms(df3)
grid.arrange(grobs=a[c(1:4)])

a <- EHSummarize_SingleColumn_Boxplots(df3)
grid.arrange(grobs=a[2])
```

#### D. Relationships

We can run a regression on total profit just to get an idea of some of the relationships between the numeric and categorical variables.  We can see from this exploration that item types are strongly correlated with profits, as are medium priority items, but nothing else is. Unit cost could not be calculated because of singularities. We know Unit Cost is not fully correlated with Total Profit, so it must be fully correlated with another variable or in conjunction with other variables.

```{r}

x <- lm(Total.Profit ~., data=df3)
summary(x)
```
This analysis suggests that Item Type may be the most reasonable class to predict.  However, region may also work, if it is correlated with some of the other variables besides profit.  We can test this conjecture with some further analysis. 

Bar charts and boxplots show relatively little relationship between region and item type, sales channel, and order priority.

```{r}
a <- EHExplore_TwoCategoricalColumns_Barcharts(df3, "Region")
grid.arrange(grobs=a[c(2)])
grid.arrange(grobs=a[c(3:4)])

```
```{r}

ggplot(df2, aes(Region, Order.Lag)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Region, OrderDaysSince2000)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Region, Unit.Cost)) +
  geom_boxplot() +
  coord_flip()

```
####. D. Choosing Item Type as the variable to predict

We therefore choose Item Type to predict for this analysis.  As with region, we can ask, "how does it correlate with the non-economic variables?"  In general, Item Type shows a similar lack of relationship to the non-economic variables as region does. But there is one major exception.  Now we see the source of the singularity - each item type has one, and only one, unit price and vice versa.  The two are completely correlated.  Just to be sure, a regression shows an R2 of 1.

```{r}

a <- EHExplore_TwoCategoricalColumns_Barcharts(df3, "Item.Type")
grid.arrange(grobs=a[c(3:4)])

```

```{r}

ggplot(df2, aes(Item.Type, Order.Lag)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Item.Type, OrderDaysSince2000)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Item.Type, Unit.Cost)) +
  geom_boxplot() +
  coord_flip()

```


```{r}
dfx3 <- df3 %>%
  dplyr::select(Unit.Cost, Item.Type)

x <- lm(Unit.Cost ~Item.Type, data=dfx3)
summary(x)

```

With Unit Cost in the analysis, a machine learning exploration is not justified, since a lookup table in Excel would perform just as well. We will retain total profit, and add Units.Sold, which has little correlation with Unit.Cost.  We remove Unit.Cost, add Units.Sold, dummify the categorical variables and scale all the predictors.

```{r}

z=list("Item.Type")

df3a <- df3 %>%
  dplyr::select(-Unit.Cost)

df3a$Units.Sold <- scale(df2$Units.Sold)

df3b <- EHPrepare_CreateDummies(df3a, exclude=z, dropFirst=TRUE)
df4 <- EHPrepare_ScaleAllButTarget(df3b, "Item.Type")


df4q <- df4 %>%
 slice(which(row_number() %% 100 == 1)) %>%
  mutate(Item.Type = ifelse(Item.Type=="Snacks"|Item.Type=="Meat"|Item.Type=="Baby Food", "Snacks", "Other"))

df5q <- df4q %>%
  dplyr::select(Total.Profit, Units.Sold, Item.Type)

write_tsv(df5q, "xqjgjk.tsv")

```


### 2. Models


#### A. Preparing the data

The data needs to be partitioned into a training set and an evaluation set. We examine our classes in the training set and see that they are relatively uniform.
```{r}
set.seed(042760)
i <- createDataPartition(df4$Item.Type, p=0.80, list=FALSE)
dfEval <- df4[-i,]
dfTrain <- df4[i,]

dfTrain %>% count(Item.Type)

```


#### B. Selecting Models

A number of factors weigh in to our decision of which models to choose.  We know that we have multiple classes to predict, that total profit, a key predictor, is not normally distributed, and that, given the strong match between item type and unit cost on the one hand and total profits and unit costs on the other, classes are likely to be relatively separate. Many of our predictors are categorical so we don't expect strong linear relationships.  The number of categories is small compared to the number of records, so our data is not sparse.  


Random Forest (RF) and multinomial regression (MR) will likely perform well under these conditions, so this is what we choose. MR has the advantages that it may be more interpretable and, because we get probabilities instead of firm classes, it is more flexible.  

We have chosen one parametric (MR) and one non-parametric method (RF). The parametric method will likely be simpler, faster and require less data.  However, it may not create as good a fit with the data.  The nonparametric method (RF) requires more data and will be slower, but will likely create a better fit.  This will lead to more accuracy, (unless the paradigm overfits the data which is more of a concern here than with MR.)


We will use 10-fold cross validation.

```{r, messages=FALSE}

tc <- trainControl(method="cv", number=10)
metric <- "Accuracy"

set.seed(042760)
multinom <- train(Item.Type~., data=dfTrain, method="multinom", metric=metric, trControl=tc)
rf <- train(Item.Type~., data=dfTrain, method="rf", metric=metric, trControl=tc)
```

```{r}

results <- resamples(list(Multinomial=multinom, RandomForest=rf))
summary(results)
dotplot(results)
```
Random Forest performs quite well.  Mean accuracy is 88% at mtry = 14.  Now we test our random forest model on the evaluation set.  We see that certain classes (beverages, fruits and personal care) are predicted very well, while others (meat, snacks) perform less well.  An analysis of why is beyond the scope of this exercise.

```{r}
print(rf)
```


#### C. Making Predicions

```{r}
predictions <- predict(rf, dfEval)
x <- factor(dfEval$Item.Type)
confusionMatrix(predictions, x)

dfPred <- as.data.frame(predictions)
ggplot(dfPred, aes(predictions)) +
  geom_bar() +
  coord_flip()
```

#### D. Analyzing the Larger Dataset

Now we examine the larger dataset and make some comparisons. Since the 5000 database is a subset of this one, we would expect many similarities. Not surprisingly, multicollinearity and distributions look the same. Unit Costs and Item Types continue to match one to one. The standard deviation of Total.Profit is slightly smaller, as is the mean.  We are not adding a lot of significant information with this data set. However, the n may improve our confidence intervals.

In fact, our accuracy improves from 87 to 98%.  All classes show 97% accuracy or higher.  This demonstrates the benefits of increasing n.

```{r}

dfLarge <- read.csv("D:\\RStudio\\CUNY_622\\1\\Salesdata_50000.csv")

summary(dfLarge)
```
```{r}

sd(df2$Total.Profit)
sd(dfLarge$Total.Profit)

mean(df2$Total.Profit)
mean(dfLarge$Total.Profit)

dfLargeNum <- dfLarge %>%
  dplyr::select_if(is.numeric)

z <- EHExplore_Multicollinearity(dfLargeNum, title="Multicollinearity Among Economic Variables")
a <- EHSummarize_SingleColumn_Histograms(dfLarge)
grid.arrange(grobs=a[c(1:7)])

ggplot(df2, aes(Item.Type, Unit.Cost)) +
  geom_boxplot() +
  coord_flip()

```
```{r}
df2 <-dfLarge

df2$Item.Type <- factor(df2$Item.Type)
df2$Region <- factor(df2$Region)

df2$Order.Date <- as.Date(df2$Order.Date, format="%m/%d/%Y")
df2$Ship.Date <- as.Date(df2$Ship.Date, format="%m/%d/%Y")

df2$Order.Lag <- as.integer(df2$Ship.Date-df2$Order.Date)
df2$OrderDaysSince2000 <- as.integer(df2$Ship.Date-as.Date("2000-01-01"))

df3 <- df2 %>%
    dplyr::select(-Order.Date, -Ship.Date, -Country, -Unit.Price, -Total.Revenue, -Total.Cost, -Units.Sold)


z=list("Item.Type")

df3a <- df3 %>%
  dplyr::select(-Unit.Cost)

df3a$Units.Sold <- scale(df2$Units.Sold)

df3b <- EHPrepare_CreateDummies(df3a, exclude=z, dropFirst=TRUE)
df4 <- EHPrepare_ScaleAllButTarget(df3b, "Item.Type")

set.seed(042760)
i <- createDataPartition(df4$Item.Type, p=0.80, list=FALSE)
dfEval <- df4[-i,]
dfTrain <- df4[i,]

dfTrain %>% count(Item.Type)

tc <- trainControl(method="cv", number=10)
metric <- "Accuracy"

set.seed(042760)
multinom <- train(Item.Type~., data=dfTrain, method="multinom", metric=metric, trControl=tc)
rf <- train(Item.Type~., data=dfTrain, method="rf", metric=metric, trControl=tc)

```

```{r}

results <- resamples(list(Multinomial=multinom, RandomForest=rf))
summary(results)
dotplot(results)

```
```{r}


predictions <- predict(rf, dfEval)
x <- factor(dfEval$Item.Type)
confusionMatrix(predictions, x)

dfPred <- as.data.frame(predictions)
ggplot(dfPred, aes(predictions)) +
  geom_bar() +
  coord_flip()
```

#### E. Conclusion

Item types were predicted from other aspects of country sales records, such as total profits, region, unit sales, priority, etc. Because each item type is associated with exactly one unit price, there is going to be 
