---
title: "Eric_Hirsch_621_Assignment_4"
subtitle: "Predicting Insurance Claims" 
author: "Eric Hirsch"
date: "4/7/2022"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =  FALSE, message = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(patchwork)
library(gridExtra)
library(ggsci)
library(caret)
library(pROC)
library(car)
library(psych)
library(patchwork)
library(tidytable)
library(MASS)
library(lubridate)
library(e1071)
library(caTools)
library(class)
#library(mice)
```
```{r}

df2 <- read.csv("D:\\RStudio\\CUNY_622\\1\\Salesdata_5000.csv")
#df2 <- read.csv("D:\\RStudio\\CUNY_622\\1\\Salesdata_50000.csv")

```

### 1. Data Exploration


#### A. Summary Statistics

For this exercise we will examine the 5000 record and 50,000 record datasets from the assignment website.

The datasets contain fabricated sales orders generated by VBA for the purpose of practicing analysis.  There are 14 columns, including 7 numeric columns, 5 character and two date. One of the variables is an ID so we drop it. Here is a summary of the remaining 13 variables:

```{r}

df2 <- df2 %>%
  dplyr::select(-Order.ID)

summary(df2)
str(df2)

df2$Item.Type <- factor(df2$Item.Type)
df2$Region <- factor(df2$Region)

```

#### B. Multicollinearity

We suspect a high degree of multicollinearity among the numeric variables, since they are components of each other - for example, total profits is made of of costs and revenues, while revenues are determined by prices and volume.  We also may assume that order and shipping dates are related, and country and region will also be related.

The heatmap below shows the multicollinearity of the economic variables.


```{r}

df2Num <- df2 %>%
  dplyr::select_if(is.numeric)

z <- EHExplore_Multicollinearity(df2Num, printCorrs = TRUE, title="Multicollinearity Among Economic Variables")


```
There are many different strategies we can take with this issue, including ignoring it.  We choose, for now, to retain Total Profit (as it summarizes most of the others), and, because the same profit may come from high revenue and high costs or low revenue and low costs, we include Unit Cost as well.  Unit cost has the lowest correlation with Total Profit of all the predictors (r=.51).

As for dates, we convert order date to an integer representing the number of days that have passed since 1/1/2000.  We also create a new variable, Order.Lag, since the difference between order date and shipping date might be predictive.

Finally, we eliminate country and retain region. This leaves us a dataframe of 8 variables.

```{r}

df2$Order.Date <- as.Date(df2$Order.Date, format="%m/%d/%Y")
df2$Ship.Date <- as.Date(df2$Ship.Date, format="%m/%d/%Y")

df2$Order.Lag <- as.integer(df2$Ship.Date-df2$Order.Date)
df2$OrderDaysSince2000 <- as.integer(df2$Ship.Date-as.Date("2000-01-01"))

df3 <- df2 %>%
    dplyr::select(-Order.Date, -Ship.Date, -Country, -Unit.Price, -Total.Revenue, -Total.Cost, -Units.Sold)
```

```{r}
summary(df3)
```
#### C. Distributions

When we examine the distributions of the numeric variables, we find that Total profit is highly skewed, total cost is somewhat skewed, and the date variables are relatively uniform.  There are many gaps in the cost distribution, as it appears to be a series of discrete values. We may consider doing a log transformation of profit if need be. Since the data is fabricated, the uniformity of the date distributions suggests to me that these dates are just pulled randomly from a uniform distribution and won't be useful.

Not surprisingly, a boxplot shows a great number of outliers for total profits - however, this is consistent with a distribution in which appear to be 

```{r}

a <- EHSummarize_SingleColumn_Histograms(df3)
grid.arrange(grobs=a[c(1:4)])

boxplot(df3$Total.Profit)
```
#### D. Relationships

We can run a regression on total profit just to get an idea of some of the relationships between the numeric and categorical variables.  We can see from this exploration that item types are strongly correlated with profits, as are medium priority items, but nothing else is. Unit cost could not be calculated because of singularities, which might also be an artifact of the fabrication process.  We know Unit Cost is not fully correlated with Total Profit, so it must be fully correlated when in conjunction with other variables.

```{r}

x <- lm(Total.Profit ~., data=df3)
summary(x)
```
This analysis suggests that Item Type may be the most reasonable class to predict.  However, region may be correlated with some of the other variables.  We can test this conjecture with some further analysis. 

Bar charts and boxplots show relatively little relationship between region and item type, sales channel, and order priority.

```{r}
a <- EHExplore_TwoCategoricalColumns_Barcharts(df3, "Region")
grid.arrange(grobs=a[c(2)])
grid.arrange(grobs=a[c(3:4)])

```
```{r}

ggplot(df2, aes(Region, Order.Lag)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Region, OrderDaysSince2000)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Region, Unit.Cost)) +
  geom_boxplot() +
  coord_flip()

```
Item Type shows a similar lack of relationship to the other variables, though there is a bit more. But there is one major exception.  Now we see the source of the singularity - each item type has one, and only one, unit price and vice versa.  The two are completely correlated.  Just to be sure, a regression shows an R2 of 1.

```{r}

a <- EHExplore_TwoCategoricalColumns_Barcharts(df3, "Item.Type")
grid.arrange(grobs=a[c(3:4)])

```

```{r}

ggplot(df2, aes(Item.Type, Order.Lag)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Item.Type, OrderDaysSince2000)) +
  geom_boxplot() +
  coord_flip()
ggplot(df2, aes(Item.Type, Unit.Cost)) +
  geom_boxplot() +
  coord_flip()

```


```{r}
dfx3 <- df3 %>%
  dplyr::select(Unit.Cost, Item.Type)

x <- lm(Unit.Cost ~Item.Type, data=dfx3)
summary(x)

```

With Unit Cost in the analysis, a machine learning exploration is not justified, since a lookup table in Excel would perform just as well. We will retain total profit, and add Units.Sold, which has little correlation with Unit.Cost.  We remove Unit.Cost, add Units.Sold, dummify the categorical variables and scale all the predictors.

```{r}

z=list("Item.Type")

df3a <- df3 %>%
  dplyr::select(-Unit.Cost)

df3a$Units.Sold <- scale(df2$Units.Sold)

df3b <- EHPrepare_CreateDummies(df3a, exclude=z, dropFirst=TRUE)
df4 <- EHPrepare_ScaleAllButTarget(df3b, "Item.Type")

```


### 2. Models


##### 1. Preparing the data

The data needs to be partitioned into a training set and an evaluation set. We examine our classes in the training set and see that they are relatively uniform.
```{r}
set.seed(042760)
i <- createDataPartition(df4$Item.Type, p=0.80, list=FALSE)
dfEval <- df4[-i,]
dfTrain <- df4[i,]

dfTrain %>% count(Item.Type)

```


##### 2. Selecting Models

A number of factors weigh in to our decision of which models to choose.  We know that we have multiple classes to predict, that total profit, a key predictor, is not normally distributed, and that, given the strong match between item type and unit cost on the one hand and total profits and unit costs on the other, classes are likely to be somewhat separate.  Random Forest and multinomial regression will likely perform well under these conditions, so this is what we choose.

We will use 10-fold cross validation.

```{r, messages=FALSE}

tc <- trainControl(method="cv", number=10)
metric <- "Accuracy"

set.seed(042760)
multinom <- train(Item.Type~., data=dfTrain, method="multinom", metric=metric, trControl=tc)
rf <- train(Item.Type~., data=dfTrain, method="rf", metric=metric, trControl=tc)
```

```{r}

results <- resamples(list(Multinomial=multinom, RandomForest=rf))
summary(results)
dotplot(results)
```
Random Forest performs quite well.  Mean accuracy is 88% at mtry = 14.  Now we test our random forest model on the evaluation set.  We see that certain classes (beverages, fruits and personal care) are predicted very well, while others (meat, snacks) perform less well.  An analysis of why is beyond the scope of this exercise.

```{r}
print(rf)
```


```{r}
predictions <- predict(rf, dfEval)
x <- factor(dfEval$Item.Type)
confusionMatrix(predictions, x)

dfPred <- as.data.frame(predictions)
ggplot(dfPred, aes(predictions)) +
  geom_bar() +
  coord_flip()
```
Now we examine the larger dataset and make some comparisons. Since the 5000 database is a subset of this one, we would expect many similarities. Not surprisingly, multicollinearity and distributions look the same. Unit Costs and Item Types continue to match one to one. The standard deviation of Total.Profit is slightly smaller, as is the mean.  We are not adding a lot of significant information with this data set. However, the n may improve our confidence intervals.

In fact, our accuracy improves from 87 to 98%.All classes show 97% accuracy or higher.  This demonstrates the benefits of increasing n.

```{r}

dfLarge <- read.csv("D:\\RStudio\\CUNY_622\\1\\Salesdata_50000.csv")

summary(dfLarge)
```
```{r}

sd(df2$Total.Profit)
sd(dfLarge$Total.Profit)

mean(df2$Total.Profit)
mean(dfLarge$Total.Profit)

dfLargeNum <- dfLarge %>%
  dplyr::select_if(is.numeric)

z <- EHExplore_Multicollinearity(dfLargeNum, title="Multicollinearity Among Economic Variables")
a <- EHSummarize_SingleColumn_Histograms(dfLarge)
grid.arrange(grobs=a[c(1:7)])

ggplot(df2, aes(Item.Type, Unit.Cost)) +
  geom_boxplot() +
  coord_flip()

```
```{r}
df2 <-dfLarge

df2$Item.Type <- factor(df2$Item.Type)
df2$Region <- factor(df2$Region)

df2$Order.Date <- as.Date(df2$Order.Date, format="%m/%d/%Y")
df2$Ship.Date <- as.Date(df2$Ship.Date, format="%m/%d/%Y")

df2$Order.Lag <- as.integer(df2$Ship.Date-df2$Order.Date)
df2$OrderDaysSince2000 <- as.integer(df2$Ship.Date-as.Date("2000-01-01"))

df3 <- df2 %>%
    dplyr::select(-Order.Date, -Ship.Date, -Country, -Unit.Price, -Total.Revenue, -Total.Cost, -Units.Sold)


z=list("Item.Type")

df3a <- df3 %>%
  dplyr::select(-Unit.Cost)

df3a$Units.Sold <- scale(df2$Units.Sold)

df3b <- EHPrepare_CreateDummies(df3a, exclude=z, dropFirst=TRUE)
df4 <- EHPrepare_ScaleAllButTarget(df3b, "Item.Type")

set.seed(042760)
i <- createDataPartition(df4$Item.Type, p=0.80, list=FALSE)
dfEval <- df4[-i,]
dfTrain <- df4[i,]

dfTrain %>% count(Item.Type)

tc <- trainControl(method="cv", number=10)
metric <- "Accuracy"

set.seed(042760)
multinom <- train(Item.Type~., data=dfTrain, method="multinom", metric=metric, trControl=tc)
rf <- train(Item.Type~., data=dfTrain, method="rf", metric=metric, trControl=tc)

```

```{r}

results <- resamples(list(Multinomial=multinom, RandomForest=rf))
summary(results)
dotplot(results)

```
```{r}


predictions <- predict(rf, dfEval)
x <- factor(dfEval$Item.Type)
confusionMatrix(predictions, x)

dfPred <- as.data.frame(predictions)
ggplot(dfPred, aes(predictions)) +
  geom_bar() +
  coord_flip()
```

